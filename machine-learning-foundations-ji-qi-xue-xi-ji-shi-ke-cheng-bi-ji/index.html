
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Machine Learning Foundations 機器學習基石 (課程筆記)</title>
    <meta name="description" content="">

    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="shortcut icon" href="../favicon.ico">

    <link rel="stylesheet" type="text/css" href="../assets/css/screen.css?v=16480e2d13">
    <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic%7COpen+Sans:700,400">
    <link rel="stylesheet" type="text/css" href="../assets/css/prism.css?v=16480e2d13">
    <link rel="canonical" href="index.html">
    <meta name="referrer" content="origin">
    
    <meta property="og:site_name" content="Hello World! I'm Vincent">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Machine Learning Foundations 機器學習基石 (課程筆記)">
    <meta property="og:description" content="由台大資工系 副教授 林軒田 提供的開放式課程, 關於教材內容皆可於 Youtube教學影片 及 投影片下載網址 取得。 Introduction 模仿人類學習的方式, 透過觀察 (data) 學習 (取出特徵、經過計算處理) 後得到有意義的技巧 (提升某項可以量化評估的表現)。 一些應用機器學習的時機 過於複雜 (不預期的狀況) 的系統不容易轉化成程式來處理 不容易寫出判定規則的系統 人尚未或無法即時判定的行為 過分客製化 (數量過多) 的反應 使用條件 具備 可理解的特徵 以及 可供量化評估改進的結果 不容易定義出規則 (容易定義則使用規則撰寫一般的程式處理即可) 有資料可供學習 Model Component Algorithm, learning algorithm...">
    <meta property="og:url" content="http://captainvincent.github.io/machine-learning-foundations-ji-qi-xue-xi-ji-shi-ke-cheng-bi-ji/">
    <meta property="article:published_time" content="2016-06-20T11:30:30.170Z">
    <meta property="article:modified_time" content="2016-06-21T14:41:04.539Z">
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Machine Learning Foundations 機器學習基石 (課程筆記)">
    <meta name="twitter:description" content="由台大資工系 副教授 林軒田 提供的開放式課程, 關於教材內容皆可於 Youtube教學影片 及 投影片下載網址 取得。 Introduction 模仿人類學習的方式, 透過觀察 (data) 學習 (取出特徵、經過計算處理) 後得到有意義的技巧 (提升某項可以量化評估的表現)。 一些應用機器學習的時機 過於複雜 (不預期的狀況) 的系統不容易轉化成程式來處理 不容易寫出判定規則的系統 人尚未或無法即時判定的行為 過分客製化 (數量過多) 的反應 使用條件 具備 可理解的特徵 以及 可供量化評估改進的結果 不容易定義出規則 (容易定義則使用規則撰寫一般的程式處理即可) 有資料可供學習 Model Component Algorithm, learning algorithm...">
    <meta name="twitter:url" content="http://captainvincent.github.io/machine-learning-foundations-ji-qi-xue-xi-ji-shi-ke-cheng-bi-ji/">
    
    <script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "Article",
    "publisher": "Hello World! I'm Vincent",
    "author": {
        "@type": "Person",
        "name": "Captain Vincent",
        "image": "http://captainvincent.github.io/content/images/2016/01/--.JPG",
        "url": "http://captainvincent.github.io/author/captain",
        "sameAs": null,
        "description": null
    },
    "headline": "Machine Learning Foundations 機器學習基石 (課程筆記)",
    "url": "http://captainvincent.github.io/machine-learning-foundations-ji-qi-xue-xi-ji-shi-ke-cheng-bi-ji/",
    "datePublished": "2016-06-20T11:30:30.170Z",
    "dateModified": "2016-06-21T14:41:04.539Z",
    "description": "由台大資工系 副教授 林軒田 提供的開放式課程, 關於教材內容皆可於 Youtube教學影片 及 投影片下載網址 取得。 Introduction 模仿人類學習的方式, 透過觀察 (data) 學習 (取出特徵、經過計算處理) 後得到有意義的技巧 (提升某項可以量化評估的表現)。 一些應用機器學習的時機 過於複雜 (不預期的狀況) 的系統不容易轉化成程式來處理 不容易寫出判定規則的系統 人尚未或無法即時判定的行為 過分客製化 (數量過多) 的反應 使用條件 具備 可理解的特徵 以及 可供量化評估改進的結果 不容易定義出規則 (容易定義則使用規則撰寫一般的程式處理即可) 有資料可供學習 Model Component Algorithm, learning algorithm..."
}
    </script>

    <meta name="generator" content="Ghost 0.7">
    <link rel="alternate" type="application/rss+xml" title="Hello World! I'm Vincent" href="../rss/index.html">
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73341093-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body class="post-template nav-closed">

    <div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="index.html#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
            <li class="nav-blog-home-page" role="presentation"><a href="../">Blog Home Page</a></li>
            <li class="nav-git-book" role="presentation"><a href="https://www.gitbook.com/@captainvincent">Git Book</a></li>
            <li class="nav-aboutme" role="presentation"><a href="https://about.me/CaptainVincent">About.Me</a></li>
    </ul>
    <a class="subscribe-button icon-feed" href="../rss/index.rss">Subscribe</a>
</div>
<span class="nav-cover"></span>


    <div class="site-wrapper">

        


<header class="main-header post-head no-cover">
    <nav class="main-nav  clearfix">
        
            <a class="menu-button icon-menu" href="index.html#"><span class="word">Menu</span></a>
    </nav>
</header>

<main class="content" role="main">
    <article class="post">

        <header class="post-header">
            <h1 class="post-title">Machine Learning Foundations 機器學習基石 (課程筆記)</h1>
            <section class="post-meta">
                <time class="post-date" datetime="2016-06-20">20 June 2016</time> 
            </section>
        </header>

        <section class="post-content">
            <blockquote>
  <p>由台大資工系 副教授 林軒田 提供的開放式課程, 關於教材內容皆可於 <a href="https://www.youtube.com/playlist?list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf">Youtube教學影片</a> 及 <a href="http://www.csie.ntu.edu.tw/~htlin/mooc/">投影片下載網址</a> 取得。</p>
</blockquote>

<h2 id="introduction">Introduction</h2>

<p>模仿人類學習的方式, 透過觀察 (data) 學習 (取出特徵、經過計算處理) 後得到有意義的技巧 (提升某項可以量化評估的表現)。</p>

<h5 id="">一些應用機器學習的時機</h5>

<ul>
<li>過於複雜 (不預期的狀況) 的系統不容易轉化成程式來處理</li>
<li>不容易寫出判定規則的系統</li>
<li>人尚未或無法即時判定的行為</li>
<li>過分客製化 (數量過多) 的反應</li>
</ul>

<h5 id="">使用條件</h5>

<ul>
<li>具備 可理解的特徵 以及 可供量化評估改進的結果</li>
<li>不容易定義出規則 (容易定義則使用規則撰寫一般的程式處理即可)</li>
<li>有資料可供學習</li>
</ul>

<h2 id="modelcomponent">Model Component</h2>

<ul>
<li><strong>A</strong>lgorithm, learning algorithm 為挑選假說的演算法</li>
<li><strong>D</strong>ata Set, D: {(<strong>X</strong><sub>1</sub>,Y<sub>1</sub>), (<strong>X</strong><sub>2</sub>,Y<sub>2</sub>), ... (<strong>X</strong><sub>N</sub>,Y<sub>N</sub>)}</li>
<li><strong>X</strong> 為 input 的特徵向量, Y 為 Output</li>
<li><strong>H</strong>ypothesis Set, 推論 <strong>X</strong> 與 Y 之間存在關係的假說集合</li>
<li><strong>f</strong> 理想上的 target function 可以完全地反應出所有的關係 (實際上 unknown), f: <strong>X</strong> → Y</li>
<li><strong>g</strong> 透過 algorithm 從 hypothesis set 中挑選出最接近 f 者</li>
<li><strong>A</strong> takes <strong>D</strong> and <strong>H</strong> to get <strong>g</strong> ≈ <strong>f</strong>, 機器學習是透過資料從假說中挑選最接近目標函式者, 用以推測訓練資料以外的其他資料結果</li>
<li>Learning Model = <strong>A</strong> and <strong>H</strong></li>
</ul>

<blockquote>
  <p><strong>Machine Learning vs Data Mining</strong></p>
  
  <p>資料探勘通常是對大量的資料中尋找有用的性質, 如果這個性質的目標是找出更好的 hypothesis, 則兩者的差異不大 或是 可相互幫忙。</p>
  
  <p><strong>Machine Learning vs Artificial Intelligence</strong></p>
  
  <p>人工智慧的目標在於經過計算之後可以得到具備智能的行為, 而機器學習是其中一種實現人工智慧的方法 (其他方法像是 決策數)。</p>
  
  <p><strong>Machine Learning vs Statistics</strong></p>
  
  <p>統計學是用資料推論一些未知的結果, 所以統計是機器學習中會使用到的其中一種工具。</p>
</blockquote>

<h2 id="perceptronlearningalgorithm">Perceptron Learning Algorithm</h2>

<p>這邊介紹一種簡單的 Hypothesis Set 的定義方式稱 "Perceptron" 來求一個是非題的解, 數學上的符號如下</p>

<p>Σ <sub>(i=1~n)</sub> w<sub>i</sub>x<sub>i</sub> (再定義出一個 threshold 來二分這是非題的結果, y = {+1 , -1})</p>

<p>h(x) = sign ( Σ <sub>(i=1~n)</sub> w<sub>i</sub>x<sub>i</sub> - threshold ) </p>

<p>令 w<sub>0</sub> 為 -threshold、x<sub>0</sub> 為 1, 則化簡如下</p>

<p>h(x) = sign ( Σ <sub>(i=0~n)</sub> w<sub>i</sub>x<sub>i</sub> ) = sign (<strong>w</strong><sup>T</sup><strong>x</strong>)</p>

<blockquote>
  <p>相當於取 w 與 x 兩個 n+1 維的向量內積。</p>
</blockquote>

<p>目標是從 H 中挑選出最接近 f 的 g, 而這最接近的定義為在已經看過的資料中可以產出愈相同的 output , 但實際上整個 H 是一個無限大的集合, 所以 <strong>PLA</strong> (Perceptron Learning Algorithm) 的想法是嘗試先從中挑選出第一個 g<sub>0</sub> (或稱 <strong>w</strong><sub>0</sub>), 並不斷的在錯誤中修正</p>

<h5 id="">演算法</h5>

<p>If, sign (<strong>w</strong><sup>T</sup><strong>x</strong><sub>n(t)</sub>) ≠ y<sub>n(t)</sub> <br>
Let, <strong>w</strong><sub>t+1</sub> = <strong>w</strong><sub>t</sub> + y<sub>n(t)</sub> <strong>x</strong><sub>n(t)</sub> <br>
Until no more mistakes.</p>

<blockquote>
  <p>想法是當計算的結果與預期的符號不同時, 表示匹配特徵向量 <strong>w</strong> 與 資料特徵向量 <strong>x</strong> 兩者間的角度過大或過小, 所以修正匹配特徵向量 <strong>w</strong> 使其遠離或靠近 <strong>x</strong> 一點 (方向視其預期結果的正負來決定)。</p>
  
  <p>sign (<strong>w</strong><sup>T</sup><strong>x</strong>) 中, <strong>w</strong><sup>T</sup><strong>x</strong> = 0 在二維中是一條法相量為 <strong>w</strong><sup>T</sup> 的直線二分其結果 y, 在高維度時則是劃分結果 y 的則是法相量 <strong>w</strong><sup>T</sup> 的高維平面。</p>
</blockquote>

<h5 id="linearseparability">Linear Separability 線性可分</h5>

<p>PLA 的終止條件是可以找出一個完全沒有錯誤的 <strong>w</strong>, 此時稱作線性可分; 而這個 <strong>w</strong><sub>f</sub>, 可以完美的劃分開不同結果的資料, 使其與分割平面 (線) 都有 &gt; 0 的距離 (因為計算結果與內積皆同向)</p>

<p>y<sub>n(t)</sub> <strong>w</strong><sub>f</sub><sup>T</sup><strong>x</strong><sub>n(t)</sub> (所有 input 包含發生錯誤的點) ≥ min( y<sub>n</sub> <strong>w</strong><sub>f</sub><sup>T</sup><strong>x</strong><sub>n</sub> ) &gt; 0</p>

<h6 id="pla">討論 PLA 演算法修正的結果好壞的判定</h6>

<p><strong>w</strong><sub>f</sub><sup>T</sup> <strong>w</strong><sub>t+1</sub> = <strong>w</strong><sub>f</sub><sup>T</sup> (<strong>w</strong><sub>t</sub> + y<sub>n(t)</sub> <strong>x</strong><sub>n(t)</sub>) &gt;= <strong>w</strong><sub>f</sub><sup>T</sup> <strong>w</strong><sub>t</sub> + min( y<sub>n</sub> <strong>w</strong><sub>f</sub><sup>T</sup><strong>x</strong><sub>n</sub> ) &gt; <strong>w</strong><sub>f</sub><sup>T</sup> <strong>w</strong><sub>t</sub></p>

<p>到此我們只證明了一半, 表示透過演算法可以不停地修正可以得到與 target 內積愈來愈大的特徵向量, 但內積愈大並不能表示兩個向量的夾角愈小 (方向愈接近), 也有可能是因為修正後的向量長度變長所造成的, 這就是我們後面所要證明的第二部分。</p>

<div>  
\begin{equation}
  \begin{split}
  w_f^Tw_T &amp;\geq w_f^Tw_{T-1} + min\ {y_nw_f^Tx_n} \\\
           &amp;\geq ... \\\
           &amp;\geq w_f^Tw_0 + T\cdot min\ {y_nw_f^Tx_n} = T\cdot min\ {y_nw_f^Tx_n}
  \end{split}
  \end{equation}
</div>  

<p>透過前式, 我們可以一路推導做了 T 次修正後如上的結果</p>

<div>  
\begin{equation}
\begin{split}
||w_{t+1}||^2 &amp;=    ||w_t + y_{n(t)}x_{n(t)}||^2 \\\
              &amp;=    ||w_t||^2 + 2y_{n(t)}w_t^Tx_{n(t)} + ||y_{n(t)}x_{n(t)}||^2\\\
              &amp;\leq ||w_t||^2 + 0 + ||y_{n(t)}x_{n(t)}||^2 \\\
              &amp;\leq ||w_t||^2 + max\ {||x_n||^2}
\end{split}
\end{equation}
</div>  

<p>因為我們僅在錯誤的時候做修正, 所以中間項的乘積會 &lt; 0, 上式就是我們得到的向量長度 Upper bound</p>

<div>  
\begin{equation}
\begin{split}
  ||w_T||^2 &amp;\leq ||w_{T-1}||^2 + max\ {||x_n||^2} \\\
            &amp;\leq ... \\\
            &amp;\leq ||w_0||^2 + T\cdot max\ {||x_n||^2} = T\cdot max\ {||x_n||^2}
\end{split}
\end{equation}
</div>  

<p>接著一樣透過前式, 我們可以一路推導做了 T 次修正後如上的結果</p>

<div>  
\begin{equation}  
\begin{split}   
1 &amp;\geq \frac{w_f^Tw_T}{||w_f||||w_T||} &amp;\geq \frac{T\cdot min\ y_nw_f^Tx_n}{||w_f||||w_T||} \ &amp;\geq \frac{T\cdot min\ y_nw_f^Tx_n}{||w_f||\cdot \sqrt{T}\cdot max\ {||x_n||^2}}  = \frac {\sqrt{T}\rho}{R}  
\end{split}    
\end{equation}
</div>  

<p>最後可以求出 cos θ 經過 T 次迭代後的收斂式子 (ρ 與 R 皆是我們導出的常數), 因此我們可知當今天的資料是 Linear Separability 時</p>

<ul>
<li>PLA 確實可修正 <strong>W</strong><sub>t</sub> 使其更加靠近 <strong>W</strong><sub>f</sub></li>
<li>由 lower bound 可以知道經過有限次的迭代後, 此演算法會中止</li>
<li>綜合以上兩點所以 PLA 可以找到一條完美的分割線</li>
</ul>

<p>得到以上的結果後, 對於 PLA 還是存在一些疑問, 包括了如何知道資料是線性可分 (<strong>W</strong><sub>f</sub> 存在), 如果是已知那實際上我們也就不需要做 PLA, 所以這部分通常是未知, 另一個問題是怎麼知道要做多久才會結束?</p>

<h5 id="noise">Noise</h5>

<p>關於雜訊會有一些假設, 我們假設大部分的時候雜訊是很少的 (雜訊過多則學雜訊的意義反而較大), 則大部分的時候 y<sub>n</sub> = f (<strong>x</strong><sub>n</sub>), 則當如果找到 g ≈ f 即得 y<sub>n</sub> = g (<strong>x</strong><sub>n</sub>)</p>

<h5 id="pocketalgorithm">Pocket Algorithm</h5>

<p>基於有效的挑選出完美的 g 其實是一個 NP-Hard 的問題, 所以這邊舉了一個簡單的演算法, 演算法的精神是建立在 PLA 不停地挑選犯錯更少的 <strong>W</strong><sub>t+1</sub> 出來, 直到夠多的迭代後以最終的結果為回傳值, 相較於 PLA 除了比對當前資料是否造成錯誤結果之外, Pocket Algorithm 要去計算所有資料的結果, 所以當今天資料是線性可分時, Pocket Algorithm 會比 PLA 慢, 但能確保有終止條件。</p>

<h2 id="">機器學習的分類</h2>

<h5 id="ysubnsub">以提供不同 y<sub>n</sub> 形式區分的學習方式</h5>

<ul>
<li>Supervised 監督式, 提供匹配的問題與解答來教學 (其他方式的基礎)</li>
<li>Unsupervised 分群的問題, 不提供 y<sub>n</sub></li>
<li>Semi-supervised 數量多無法窮舉時, 部分標記剩餘透過機器學習分類</li>
<li>Reinforcement 單筆單筆的告知系統反應是 好 或是 不好, 不一定精確知道其輸入對應的輸出為何, 但有輔助判定的資訊</li>
</ul>

<h5 id="">以提供資料方式區分的種類</h5>

<ul>
<li>Batch 成批的資料訓練</li>
<li>Online, sequentially 一個一個的逐步改善 </li>
<li>Active, sequentially 機器主動的方式來自動挑出盲點 (應用於標記資料需要高成本)</li>
</ul>

<h5 id="">以資料內容區分的種類</h5>

<ul>
<li>Concrete Features 具體有關聯性的特徵 (需有 domain knowledge)</li>
<li>Raw Features 通常需要人或機器建立、抽取出具體的特徵, 以避免原始資料對於問題過於抽象</li>
<li>Abstract Features 輸入資料完全不具任何意義, 同 raw features 一樣需要建立出特徵</li>
</ul>
        </section>

        <footer class="post-footer">


            <figure class="author-image">
                <a class="img" href="../author/captain/" style="background-image: url(../content/images/2016/01/--.JPG)"><span class="hidden">Captain Vincent's Picture</span></a>
            </figure>

            <section class="author">
                <h4><a href="../author/captain/">Captain Vincent</a></h4>

                    <p>Read <a href="../author/captain/">more posts</a> by this author.</p>
                <div class="author-meta">
                    
                    
                </div>
            </section>


            <section class="share">
                <h4>Share this post</h4>
                <a class="icon-twitter" href="https://twitter.com/intent/tweet?text=Machine%20Learning%20Foundations%20%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%9F%BA%E7%9F%B3%20(%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98)&amp;url=http://captainvincent.github.io/machine-learning-foundations-ji-qi-xue-xi-ji-shi-ke-cheng-bi-ji/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <span class="hidden">Twitter</span>
                </a>
                <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=http://captainvincent.github.io/machine-learning-foundations-ji-qi-xue-xi-ji-shi-ke-cheng-bi-ji/index.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <span class="hidden">Facebook</span>
                </a>
                <a class="icon-google-plus" href="https://plus.google.com/share?url=http://captainvincent.github.io/machine-learning-foundations-ji-qi-xue-xi-ji-shi-ke-cheng-bi-ji/" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <span class="hidden">Google+</span>
                </a>
            </section>

            <div id="disqus_thread"></div>
            <script>
            /**
            * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
            * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
            */

            var disqus_config = function () {
                this.page.url = 'http://captainvincent.github.io/machine-learning-foundations-ji-qi-xue-xi-ji-shi-ke-cheng-bi-ji/'; // Replace PAGE_URL with your page's canonical URL variable
                this.page.identifier = 'ghost-30'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
            };

            (function() { // DON'T EDIT BELOW THIS LINE
                var d = document, s = d.createElement('script');

                s.src = '//captainvincent.disqus.com/embed.js';

                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);
            })();
            </script>
            <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

        </footer>

    </article>
</main>

<aside class="read-next">
    <a class="read-next-story prev no-cover" href="../dan-ni-er-bo-shi-pao-bu-fang-cheng-shi/">
        <section class="post">
            <h2>丹尼爾博士 跑步方程式 筆記</h2>
            <p>原名      Daniel's Running Formula. - 3rd Edition by Jack T. Daniels 開始翻閱這本書的理由, 是由於筆者開始意識到了自己該重新鍛鍊身體的歲數了, 過去著重於重量訓練的自己, 在找回自己這方面決定從跑步開始, 先提升自己的心肺功能再開始高強度的訓練, 這就是筆者的初衷。 跑者成功的要素 才能、…</p>
        </section>
    </a>
</aside>




        <footer class="site-footer clearfix">
            <section class="copyright"><a href="../">Hello World! I'm Vincent</a> © 2016</section>
            <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
        </footer>

    </div>

    <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.3.min.js"></script>
    

    <script type="text/javascript" src="../assets/js/jquery.fitvids.js?v=16480e2d13"></script>
    <script type="text/javascript" src="../assets/js/prism.js?v=16480e2d13"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$','$'], ['\\\\(','\\\\)']],
                processEscapes: true
            }
        });
    </script>
    <script type="text/javascript" src="../assets/js/index.js?v=16480e2d13"></script>

</body>
