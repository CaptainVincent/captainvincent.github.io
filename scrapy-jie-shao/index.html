
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Scrapy 介紹</title>
    <meta name="description" content="">

    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="shortcut icon" href="../favicon.ico">

    <link rel="stylesheet" type="text/css" href="../assets/css/screen.css?v=8866cd0de0">
    <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic%7COpen+Sans:700,400">
    <link rel="stylesheet" type="text/css" href="../assets/css/prism.css?v=8866cd0de0">

    <link rel="canonical" href="index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="amphtml" href="amp/index.html">
    
    <meta property="og:site_name" content="Hello World! I'm Vincent.">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Scrapy 介紹">
    <meta property="og:description" content="延續前一個主題 CachedPage 的工作後, 有了目標網站在 Cached Page Server 上還存留的清單, 接下來要進行較為複雜的爬蟲工作 (解析原始網頁內容、抓取圖片), 這時候之前學會的技巧就遇到了一些問題   之前的架構 是循序, 透過 Requests 抓取 html content 解析網址, 再使用 BeautifulSoup, bs4 去將 html 轉成結構化的物件, 透過此 lib 的協助提取所需要的內文資訊。 Performance 循序的方式會造成因為 I/O wait 的時間太久, 導致需要連續 I/O 的工作大部分的時間都花在等待。 Data 大量蒐集的資料要如何存放, 以方便為後續作業上的使用保留彈性。Next Topic Official Tutorial dataflow reference form The">
    <meta property="og:url" content="http://captainvincent.github.io/scrapy-jie-shao/">
    <meta property="article:published_time" content="2016-12-28T21:41:05.000Z">
    <meta property="article:modified_time" content="2017-01-02T20:25:46.000Z">
    <meta property="article:publisher" content="https://www.facebook.com/UndercoverEngineer">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Scrapy 介紹">
    <meta name="twitter:description" content="延續前一個主題 CachedPage 的工作後, 有了目標網站在 Cached Page Server 上還存留的清單, 接下來要進行較為複雜的爬蟲工作 (解析原始網頁內容、抓取圖片), 這時候之前學會的技巧就遇到了一些問題   之前的架構 是循序, 透過 Requests 抓取 html content 解析網址, 再使用 BeautifulSoup, bs4 去將 html 轉成結構化的物件, 透過此 lib 的協助提取所需要的內文資訊。 Performance 循序的方式會造成因為 I/O wait 的時間太久, 導致需要連續 I/O 的工作大部分的時間都花在等待。 Data 大量蒐集的資料要如何存放, 以方便為後續作業上的使用保留彈性。Next Topic Official Tutorial dataflow reference form The">
    <meta name="twitter:url" content="http://captainvincent.github.io/scrapy-jie-shao/">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Captain Vincent">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Hello World! I&#x27;m Vincent.",
        "logo": "http://captainvincent.github.io/ghost/img/ghosticon.jpg"
    },
    "author": {
        "@type": "Person",
        "name": "Captain Vincent",
        "image": {
            "@type": "ImageObject",
            "url": "http://captainvincent.github.io/content/images/2016/11/--.JPG",
            "width": 889,
            "height": 1080
        },
        "url": "http://captainvincent.github.io/author/vincent/",
        "sameAs": []
    },
    "headline": "Scrapy 介紹",
    "url": "http://captainvincent.github.io/scrapy-jie-shao/",
    "datePublished": "2016-12-28T21:41:05.000Z",
    "dateModified": "2017-01-02T20:25:46.000Z",
    "description": "延續前一個主題 CachedPage 的工作後, 有了目標網站在 Cached Page Server 上還存留的清單, 接下來要進行較為複雜的爬蟲工作 (解析原始網頁內容、抓取圖片), 這時候之前學會的技巧就遇到了一些問題   之前的架構 是循序, 透過 Requests 抓取 html content 解析網址, 再使用 BeautifulSoup, bs4 去將 html 轉成結構化的物件, 透過此 lib 的協助提取所需要的內文資訊。 Performance 循序的方式會造成因為 I/O wait 的時間太久, 導致需要連續 I/O 的工作大部分的時間都花在等待。 Data 大量蒐集的資料要如何存放, 以方便為後續作業上的使用保留彈性。Next Topic Official Tutorial dataflow reference form The",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://captainvincent.github.io"
    }
}
    </script>

    <meta name="generator" content="Ghost 0.11">
    <link rel="alternate" type="application/rss+xml" title="Hello World! I'm Vincent." href="../rss/index.html">
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73341093-1', 'auto');
  ga('send', 'pageview');
</script>
</head>
<body class="post-template nav-closed">

    <div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="index.html#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
            <li class="nav-blog-home-page" role="presentation"><a href="../">Blog Home Page</a></li>
            <li class="nav-git-book" role="presentation"><a href="https://www.gitbook.com/@captainvincent">Git Book</a></li>
            <li class="nav-aboutme" role="presentation"><a href="https://about.me/CaptainVincent">About.Me</a></li>
    </ul>
        <a class="subscribe-button icon-feed" href="../rss/index.rss">Subscribe</a>
</div>
<span class="nav-cover"></span>


    <div class="site-wrapper">

        


<header class="main-header post-head no-cover">
    <nav class="main-nav  clearfix">
        
            <a class="menu-button icon-menu" href="index.html#"><span class="word">Menu</span></a>
    </nav>
</header>

<main class="content" role="main">
    <article class="post">

        <header class="post-header">
            <h1 class="post-title">Scrapy 介紹</h1>
            <section class="post-meta">
                <time class="post-date" datetime="2016-12-29">29 December 2016</time> 
            </section>
        </header>

        <section class="post-content">
            <p>延續前一個主題 <a href="../cached-page/">CachedPage</a> 的工作後, 有了目標網站在 Cached Page Server 上還存留的清單, 接下來要進行較為複雜的爬蟲工作 (解析原始網頁內容、抓取圖片), 這時候之前學會的技巧就遇到了一些問題</p>

<blockquote>
  <p><a href="https://github.com/CaptainVincent/CachedPageSeedFinder">之前的架構</a> 是循序, 透過 <a href="http://docs.python-requests.org/en/master/">Requests</a> 抓取 html content 解析網址, 再使用 <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup, bs4</a> 去將 html 轉成結構化的物件, 透過此 lib 的協助提取所需要的內文資訊。</p>
</blockquote>

<ul>
<li><strong>Performance</strong> 循序的方式會造成因為 I/O wait 的時間太久, 導致需要連續 I/O 的工作大部分的時間都花在等待。</li>
<li><strong>Data</strong> 大量蒐集的資料要如何存放, 以方便為後續作業上的使用保留彈性。<a href="../pymongo-jie-shao/">Next Topic</a></li>
</ul>

<h4 id="officialtutorialhttpsdocscrapyorgenlatestintrotutorialhtml"><a href="https://doc.scrapy.org/en/latest/intro/tutorial.html">Official Tutorial</a></h4>

<p><img src="http://3.bp.blogspot.com/-TRxU3bGgqAY/Vd_L56GQMRI/AAAAAAAAAhk/lS7mSF_25wQ/s1600/Scrapy-Dataflow.png" alt="Data flow">
<a href="http://itcprogblog.blogspot.tw/2015/08/scraping-with-scrapy.html">dataflow reference</a> form <a href="http://itcprogblog.blogspot.tw/">The ITC Prog Blog</a></p>

<p>Brief Overview</p>

<ul>
<li>The spider schedules requests for download (1)</li>
<li>The engine manages these requests and then executes them concurrently (2,3)</li>
<li>When the response is available (4) the engine sends it back to the spider (5,6)</li>
<li>The spider then scrapes the response (7) and either returns more requests to the scheduler (1), or returns scraped items from the webpage (8a,8b).</li>
</ul>

<p>從上圖可以知道, 只有左半邊是我們要處理的事情。</p>

<hr>

<h2 id="createproject">Create Project</h2>

<pre><code class="language- language-bash">scrapy startproject MySpider  
</code></pre>

<p>MySpider 為新建的 project name, 可自行命名</p>

<pre><code>MySpider  
|-- MySpider
|   |-- __init__.py
|   |-- items.py
|   |-- middlewares.py
|   |-- pipelines.py
|   |-- settings.py
|   `-- spiders
|       `-- __init__.py
`-- scrapy.cfg
</code></pre>

<p>筆者目前 project 使用到的檔案</p>

<ul>
<li>spiders/<code>&lt;spider name&gt;</code></li>
<li>items.py</li>
<li>pipelines.py</li>
<li>settings.py</li>
</ul>

<h3 id="spider">Spider</h3>

<p>spiders/exmaple.py  </p>

<pre><code class="language- language-python">import scrapy

class MySpider(scrapy.Spider):  
    name = 'myspider'
    #allowed_domains = ['domain url']
    start_urls = [
        'http://www.google.com.tw',
    ]

    def parse(self, response):
         (...)
         yield scrapy.Request(url, callback)
</code></pre>

<ul>
<li>name 是用來定義執行時的名稱 ex. <code>scrapy crawl myspider</code></li>
<li>allowed_domains 用來限制允許的連線</li>
<li>start_urls 起始抓取的頁面, 得到 response 會呼叫 parse 這支 callback</li>
<li>parse 從 start_urls 得到 response, 可將此視為爬蟲主程式的進入點, 將 response 的內容提取出下一步要爬取的網址, 再透過 yield 送出 request, 而 requset 取得的內容可自行定義連結至哪個 function 來作為 callback</li>
</ul>

<h3 id="item">Item</h3>

<p>item 為 scrapy framework 中用來串接 pipeline 動作的資料結構, 在 item.py 中先定義出此資料的內容</p>

<p>item.py  </p>

<pre><code class="language- language-python">class MyspiderItem(scrapy.Item):  
    # define the fields for your item here like:
    person = scrapy.Field()
    image_urls = scrapy.Field()
    images = scrapy.Field()
</code></pre>

<ul>
<li>person 為筆者示範如何自行定義欄位, 可透過相同的宣告方式來新增多個資料存放欄位, 但這裡 <strong>image_urls、images</strong> 為特殊欄位, 當 item 宣告這兩個欄位並在 settings.py 中去 enable ImagesPipeline, 便可以在爬蟲執行過程中透過對 image_urls list 加入 image url, 讓 scrapy 自行下載 (透過內建的 ImagesPipeline) 並把下載完的狀態存放至 images 這個 list 之中。</li>
</ul>

<h3 id="pipeline">Pipeline</h3>

<p>Pipeline 用來對 spider 撈出的資料 (要先存放至 item 中) 進行進一步的操作, ex. 寫入資料庫</p>

<p>pipelines.py  </p>

<pre><code class="language- language-python">class MyspiderPipeline(object):  
    def open_spider(self, spider):
        (...)
    def close_spider(self, spider):
        (...)
    def process_item(self, item, spider):
        (...)
</code></pre>

<p>這裡介紹三個 stage</p>

<ul>
<li>open_spider spider 開啟時會觸發, 可以在這個階段做連接資料庫的動作</li>
<li>close_spider spider 關閉時觸發, 可以在這個階段記錄一些 log (ex. 爬蟲執行的結果)</li>
<li>process_item 當 spider 執行時有建立 item 物件, 離開該執行區塊時會觸發 (表示該 item 資料被寫入完畢), 可以在此階段將該 item 上的資訊寫入資料庫</li>
</ul>

<hr>

<p>將上述三個部分的程式實作完成後, 還是有一些小地方要修改才跑得起來, 這邊一一介紹</p>

<h3 id="robotstxt">robots.txt</h3>

<p>Scarpy default 遵守 Robots exclusion standard, 會先嘗試在 domain 中尋找 robots.txt 的存在才允許進行 crawl, 要將此行為關掉, 修改 settings.py 如下</p>

<p>settings.py  </p>

<pre><code class="language- language-python"># Obey robots.txt rules
ROBOTSTXT_OBEY = False  
</code></pre>

<h3 id="enabledpipeline">Enabled Pipeline</h3>

<p>如果有自行新增的 pileline 要執行, 或是希望圖片可以透過 scrapy 自行下載 (如上面 item 範例中所述), 就要在這邊告知 scrapy, 一樣是修改 settings.py</p>

<p>settings.py  </p>

<pre><code class="language- language-python">ITEM_PIPELINES = {  
    'scrapy.pipelines.images.ImagesPipeline': 1,
    'MySpider.pipelines.MySpiderPipeline': 300,
}
</code></pre>

<blockquote>
  <p>後面的數字愈小, 執行的順序愈優先; ImagesPipeline 是 Scrapy 內建的, 沒有特殊的需求可以直接使用此 pipeline。</p>
</blockquote>

<p>以上就是改用 Scrapy 來作為爬蟲框架的重點整理, 有任何使用上的疑問或說明上的錯誤都歡迎留言給筆者。</p>
        </section>

        <footer class="post-footer">


            <figure class="author-image">
                <a class="img" href="../author/vincent/" style="background-image: url(../content/images/2016/11/--.JPG)"><span class="hidden">Captain Vincent's Picture</span></a>
            </figure>

            <section class="author">
                <h4><a href="../author/vincent/">Captain Vincent</a></h4>

                    <p>Read <a href="../author/vincent/">more posts</a> by this author.</p>
                <div class="author-meta">
                    
                    
                </div>
            </section>


            <section class="share">
                <h4>Share this post</h4>
                <a class="icon-twitter" href="https://twitter.com/intent/tweet?text=Scrapy%20%E4%BB%8B%E7%B4%B9&amp;url=http://captainvincent.github.io/scrapy-jie-shao/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <span class="hidden">Twitter</span>
                </a>
                <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=http://captainvincent.github.io/scrapy-jie-shao/" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <span class="hidden">Facebook</span>
                </a>
                <a class="icon-google-plus" href="https://plus.google.com/share?url=http://captainvincent.github.io/scrapy-jie-shao/" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <span class="hidden">Google+</span>
                </a>
            </section>


            <div id="disqus_thread"></div>
            <script>
            /**
            * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
            * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
            */

            var disqus_config = function () {
                this.page.url = 'http://captainvincent.github.io/scrapy-jie-shao/'; // Replace PAGE_URL with your page's canonical URL variable
                this.page.identifier = 'ghost-48'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
            };

            (function() { // DON'T EDIT BELOW THIS LINE
                var d = document, s = d.createElement('script');

                s.src = '//captainvincent.disqus.com/embed.js';

                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);
            })();
            </script>
            <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

        </footer>

    </article>
</main>

<aside class="read-next">
    <a class="read-next-story no-cover" href="../pymongo-jie-shao/">
        <section class="post">
            <h2>pymongo 介紹 (未完)</h2>
            <p>…</p>
        </section>
    </a>
    <a class="read-next-story prev no-cover" href="../cached-page/">
        <section class="post">
            <h2>Cached Page</h2>
            <p>筆者最近的功課之一是需要撈一個已經下線的網站內容, 所以首先要從網路上的頁面庫存下手 cachedpages; 順帶一提, 撰寫爬蟲撈取資料的人很多, 一種使用 cached page 的方式是因為目標網站會 ban 掉一些 (惡意?) 造成頻寬問題的連續需求發送端, 所以轉而向 cached page 著手。 Github Source…</p>
        </section>
    </a>
</aside>



        <footer class="site-footer clearfix">
            <section class="copyright"><a href="../">Hello World! I'm Vincent.</a> © 2017</section>
            <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
        </footer>

    </div>

    <script type="text/javascript" src="http://code.jquery.com/jquery-1.12.0.min.js"></script>
    
    <script type="text/javascript" src="../assets/js/jquery.fitvids.js?v=8866cd0de0"></script>
    <script type="text/javascript" src="../assets/js/index.js?v=8866cd0de0"></script>
    <script type="text/javascript" src="../assets/js/prism.js?v=8866cd0de0"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$','$'], ['\\\\(','\\\\)']],
                processEscapes: true
            }
        });
    </script>
</body>
