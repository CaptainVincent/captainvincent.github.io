
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Cached Page</title>
    <meta name="description" content="">

    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="shortcut icon" href="../favicon.ico">

    <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic%7COpen+Sans:700,400">
    <link rel="stylesheet" type="text/css" href="../assets/css/screen.css?v=9347063cef">


    <link rel="canonical" href="index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="amphtml" href="amp/index.html">
    
    <meta property="og:site_name" content="Hello World! I'm Vincent.">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Cached Page">
    <meta property="og:description" content="筆者最近的功課之一是需要撈一個已經下線的網站內容, 所以首先要從網路上的頁面庫存下手 cachedpages; 順帶一提, 撰寫爬蟲撈取資料的人很多, 一種使用 cached page 的方式是因為目標網站會 ban 掉一些 (惡意?) 造成頻寬、系統負擔問題的連續需求發送端, 所以轉而向 cached page 著手。 Github Source 這裡因為 google 的頁面庫存保留時間過短, 所以實作上僅對 https://archive.org/web/ 做查詢, 用來幫助蒐集仍存留在  cached page server 上的網址有哪些, 使用方式如下。(當然更辛苦的是針對這些不同時期 mirror 下來的頁面內容做 parsing T.T) python CachedPageSeedFinder.py domain &amp;lt;domain&amp;gt; [--output=&amp;lt;">
    <meta property="og:url" content="http://captainvincent.github.io/cached-page/">
    <meta property="article:published_time" content="2016-12-24T23:22:40.000Z">
    <meta property="article:modified_time" content="2017-01-03T12:25:50.000Z">
    <meta property="article:tag" content="python">
    <meta property="article:tag" content="crawler">
    <meta property="article:tag" content="cachedpage">
    <meta property="article:tag" content="docopt">
    <meta property="article:tag" content="requests">
    <meta property="article:tag" content="bs4">
    
    <meta property="article:publisher" content="https://www.facebook.com/UndercoverEngineer">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Cached Page">
    <meta name="twitter:description" content="筆者最近的功課之一是需要撈一個已經下線的網站內容, 所以首先要從網路上的頁面庫存下手 cachedpages; 順帶一提, 撰寫爬蟲撈取資料的人很多, 一種使用 cached page 的方式是因為目標網站會 ban 掉一些 (惡意?) 造成頻寬、系統負擔問題的連續需求發送端, 所以轉而向 cached page 著手。 Github Source 這裡因為 google 的頁面庫存保留時間過短, 所以實作上僅對 https://archive.org/web/ 做查詢, 用來幫助蒐集仍存留在  cached page server 上的網址有哪些, 使用方式如下。(當然更辛苦的是針對這些不同時期 mirror 下來的頁面內容做 parsing T.T) python CachedPageSeedFinder.py domain &amp;lt;domain&amp;gt; [--output=&amp;lt;">
    <meta name="twitter:url" content="http://captainvincent.github.io/cached-page/">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="CaptainVincent">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="python, crawler, cachedpage, docopt, requests, bs4">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Hello World! I&#x27;m Vincent.",
        "logo": "http://captainvincent.github.io/ghost/img/ghosticon.jpg"
    },
    "author": {
        "@type": "Person",
        "name": "CaptainVincent",
        "image": {
            "@type": "ImageObject",
            "url": "http://captainvincent.github.io/content/images/2017/03/--.JPG",
            "width": 889,
            "height": 1080
        },
        "url": "http://captainvincent.github.io/author/captainvincent/",
        "sameAs": []
    },
    "headline": "Cached Page",
    "url": "http://captainvincent.github.io/cached-page/",
    "datePublished": "2016-12-24T23:22:40.000Z",
    "dateModified": "2017-01-03T12:25:50.000Z",
    "keywords": "python, crawler, cachedpage, docopt, requests, bs4",
    "description": "筆者最近的功課之一是需要撈一個已經下線的網站內容, 所以首先要從網路上的頁面庫存下手 cachedpages; 順帶一提, 撰寫爬蟲撈取資料的人很多, 一種使用 cached page 的方式是因為目標網站會 ban 掉一些 (惡意?) 造成頻寬、系統負擔問題的連續需求發送端, 所以轉而向 cached page 著手。 Github Source 這裡因為 google 的頁面庫存保留時間過短, 所以實作上僅對 https://archive.org/web/ 做查詢, 用來幫助蒐集仍存留在  cached page server 上的網址有哪些, 使用方式如下。(當然更辛苦的是針對這些不同時期 mirror 下來的頁面內容做 parsing T.T) python CachedPageSeedFinder.py domain &amp;lt;domain&amp;gt; [--output&#x3D;&amp;lt;",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://captainvincent.github.io"
    }
}
    </script>

    <meta name="generator" content="Ghost 0.11">
    <link rel="alternate" type="application/rss+xml" title="Hello World! I'm Vincent." href="../rss/index.html">
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73341093-1', 'auto');
  ga('send', 'pageview');
</script>
</head>
<body class="post-template tag-python tag-crawler tag-cachedpage tag-docopt tag-requests tag-bs4 nav-closed">

    <div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="index.html#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
            <li class="nav-blog-home-page"><a href="../">Blog Home Page</a></li>
            <li class="nav-git-book"><a href="https://www.gitbook.com/@captainvincent">Git Book</a></li>
            <li class="nav-aboutme"><a href="https://about.me/CaptainVincent">About.Me</a></li>
    </ul>
        <a class="subscribe-button icon-feed" href="../rss/index.rss">Subscribe</a>
</div>
<span class="nav-cover"></span>


    <div class="site-wrapper">

        


<header class="main-header post-head no-cover">
    <nav class="main-nav  clearfix">
        
            <a class="menu-button icon-menu" href="index.html#"><span class="word">Menu</span></a>
    </nav>
</header>

<main class="content" role="main">
    <article class="post tag-python tag-crawler tag-cachedpage tag-docopt tag-requests tag-bs4">

        <header class="post-header">
            <h1 class="post-title">Cached Page</h1>
            <section class="post-meta">
                <time class="post-date" datetime="2016-12-25">25 December 2016</time>  on <a href="../tag/python/">python</a>, <a href="../tag/crawler/">crawler</a>, <a href="../tag/cachedpage/">cachedpage</a>, <a href="../tag/docopt/">docopt</a>, <a href="../tag/requests/">requests</a>, <a href="../tag/bs4/">bs4</a>
            </section>
        </header>

        <section class="post-content">
            <p>筆者最近的功課之一是需要撈一個已經下線的網站內容, 所以首先要從網路上的頁面庫存下手 <a href="http://www.cachedpages.com/">cachedpages</a>; 順帶一提, 撰寫爬蟲撈取資料的人很多, 一種使用 cached page 的方式是因為目標網站會 ban 掉一些 (惡意?) 造成頻寬、系統負擔問題的連續需求發送端, 所以轉而向 cached page 著手。</p>

<p><a href="https://github.com/CaptainVincent/CachedPageSeedFinder">Github Source</a></p>

<p>這裡因為 google 的頁面庫存保留時間過短, 所以實作上僅對 <a href="https://archive.org/web/">https://archive.org/web/</a> 做查詢, 用來幫助蒐集仍存留在  cached page server 上的網址有哪些, 使用方式如下。(當然更辛苦的是針對這些不同時期 mirror 下來的頁面內容做 parsing T.T)</p>

<pre><code class="language-bash">python CachedPageSeedFinder.py domain &lt;domain&gt; [--output=&lt;ofname&gt;] [--input=&lt;dfname&gt;] [--ignore=&lt;list&gt;]  
</code></pre>

<h4 id="options">Options:</h4>

<ul>
<li><code>-h --help</code> Show this screen.</li>
<li><code>--version</code> Show version.</li>
<li><code>--output=&lt;ofname&gt;</code> Output all links to a file</li>
<li><code>--ignore=&lt;list&gt;</code> Ignore url with filter list. Split input use separator ','. ex. --ignore=key1,key2</li>
<li><code>--history</code> Get whole domain link with all history</li>
<li><code>--debugfile=&lt;dfname&gt;</code> Debug only. Test parse link function use an input file.</li>
</ul>

<p>實作的內容很簡單, 先是對 cached page server 送出 query 的網址查詢後, 將回傳的內容做解析撈出網址即可, 算是簡易的 crawler, 有興趣可以直接參考 <a href="https://github.com/CaptainVincent/CachedPageSeedFinder">Github Source</a>。</p>

<h4 id="pythonmodule">使用到的 python module</h4>

<ul>
<li>docopt 提供命令列解析的工具</li>
<li>requests 用來對 server 發送 request</li>
<li>bs4 (BeautifulSoup 4) 用來解析 html 的內容</li>
</ul>

<blockquote>
  <p>題外話</p>
  
  <ol>
  <li>在功能實作上是不需要使用到 docopt, 但無意間看到介紹的相關文章, 也就用來當作練習的一部份。</li>
  <li>後續要進行大量的解析這些不同時期殘留的庫存頁面、以及圖片的抓取, 已經改用 scrapy 的框架來實作, 主要是對於大量需要花時間的 io (Internet 連線) 工作, 單一循序的執行等待過於費時, 透過 scrapy 能同時進行多個非同步的 request 達到改善, 另外在 html 的解析上也改用 lxml (理由是依據 StackOverflow 上 <a href="http://stackoverflow.com/questions/8342335/xpath-vs-dom-vs-beautifulsoup-vs-lxml-vs-other-which-is-the-fastest-approach-to">效能的評估</a>)。</li>
  </ol>
</blockquote>
        </section>

        <footer class="post-footer">


            <figure class="author-image">
                <a class="img" href="../author/captainvincent/" style="background-image: url(../content/images/2017/03/--.JPG)"><span class="hidden">CaptainVincent's Picture</span></a>
            </figure>

            <section class="author">
                <h4><a href="../author/captainvincent/">CaptainVincent</a></h4>

                    <p>Read <a href="../author/captainvincent/">more posts</a> by this author.</p>
                <div class="author-meta">
                    
                    
                </div>
            </section>


            <section class="share">
                <h4>Share this post</h4>
                <a class="icon-twitter" href="https://twitter.com/intent/tweet?text=Cached%20Page&amp;url=http://captainvincent.github.io/cached-page/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <span class="hidden">Twitter</span>
                </a>
                <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=http://captainvincent.github.io/cached-page/" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <span class="hidden">Facebook</span>
                </a>
                <a class="icon-google-plus" href="https://plus.google.com/share?url=http://captainvincent.github.io/cached-page/" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <span class="hidden">Google+</span>
                </a>
            </section>


        </footer>

    </article>
</main>

<aside class="read-next">
    <a class="read-next-story no-cover" href="../scrapy-jie-shao/">
        <section class="post">
            <h2>Scrapy 介紹</h2>
            <p>延續前一個主題針對 CachedPage 做 crawl, 我有了目標網站在 Cached Page Server 上還存留的清單, 接下來要進行較為複雜的爬蟲工作 (反覆的解析原始網頁內容、抓取圖片), 這時候之前學會的技巧就遇到了一些問題   之前的架構 是循序, 透過 Requests 抓取 html…</p>
        </section>
    </a>
    <a class="read-next-story prev no-cover" href="../tradingbot-speech/">
        <section class="post">
            <h2>TradingBot Speech</h2>
            <p>講者 Philipz 鄭淳尹 ＠東吳大學      題外話, 講者在早期參加創業競賽, 當時認識了 VoiceTube 的創辦人詹益維。 Slide Link 目前專業仍是放在資訊背景中的系統架構為主, 提到關於未來技術的 keyword  infrastructure as code 讓大家自己 survey。…</p>
        </section>
    </a>
</aside>



        <footer class="site-footer clearfix">
            <section class="copyright"><a href="../">Hello World! I'm Vincent.</a> © 2017</section>
            <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
        </footer>

    </div>

    <script type="text/javascript" src="http://code.jquery.com/jquery-1.12.0.min.js"></script>
    
    <script type="text/javascript" src="../assets/js/jquery.fitvids.js?v=9347063cef"></script>
    <script type="text/javascript" src="../assets/js/index.js?v=9347063cef"></script>

</body>
